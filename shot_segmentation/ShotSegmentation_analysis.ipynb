{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import glob\n",
    "import math\n",
    "import collections\n",
    "from shutil import copyfile\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/Users/nalingupta/Documents/Documents /NEU/DS5500 Capstone Project/Project/Test Videos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShotSegmentation:\n",
    "    def __init__(self, frames_dir):\n",
    "        self.frames_dir = frames_dir\n",
    "        self.frame_ids, self.frames = self.get_frames()\n",
    "        self.encoded_frames = None\n",
    "        \n",
    "        self.tsne_X = None\n",
    "        self.db = None\n",
    "    \n",
    "    def get_frames(self):\n",
    "        frames = []\n",
    "        frame_ids = sorted(glob.glob(os.path.join(self.frames_dir, '*.png')))\n",
    "        \n",
    "        for frame_id in frame_ids:\n",
    "            frame = load_img(frame_id, target_size=(224, 224))\n",
    "            frame = img_to_array(frame)\n",
    "            frame = preprocess_input(frame)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        return frame_ids, np.array(frames)\n",
    "    \n",
    "    def get_clusters(self, perplexity=50, eps=7, min_samples=10):\n",
    "        # Encoding frames using VGG16\n",
    "        vgg16 = VGG16()\n",
    "        vgg16 = Model(inputs=vgg16.inputs, outputs=vgg16.layers[-2].output)\n",
    "        \n",
    "        self.encoded_frames = vgg16.predict(self.frames)\n",
    "        \n",
    "        print('> Finished encoding frames using VGG16')\n",
    "        \n",
    "        # TSNE Reduction\n",
    "        tsne = TSNE(n_components=2, init='random',\n",
    "                         random_state=0, perplexity=perplexity)\n",
    "        self.tsne_X = tsne.fit_transform(self.encoded_frames)\n",
    "        \n",
    "        print('> Performed dimensionality reduction using TSNE')\n",
    "        \n",
    "        # DBSCAN clustering\n",
    "        self.db = DBSCAN(eps=7, min_samples=10).fit(self.tsne_X)\n",
    "        \n",
    "        print('> Created clusters using DBSCAN')\n",
    "        \n",
    "        return self.db.labels_\n",
    "    \n",
    "    def dbscan_plot(self):\n",
    "        core_samples_mask = np.zeros_like(self.db.labels_, dtype=bool)\n",
    "        core_samples_mask[self.db.core_sample_indices_] = True\n",
    "        labels = self.db.labels_\n",
    "\n",
    "        # Number of clusters in labels, ignoring noise if present.\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise_ = list(labels).count(-1)\n",
    "        \n",
    "        unique_labels = set(labels)\n",
    "        colors = [plt.cm.Spectral(each)\n",
    "                  for each in np.linspace(0, 1, len(unique_labels))]\n",
    "        for k, col in zip(unique_labels, colors):\n",
    "            if k == -1:\n",
    "                # Black used for noise.\n",
    "                col = [0, 0, 0, 1]\n",
    "\n",
    "            class_member_mask = (labels == k)\n",
    "\n",
    "            xy = self.tsne_X[class_member_mask & core_samples_mask]\n",
    "            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                     markeredgecolor='k', markersize=14)\n",
    "\n",
    "            xy = self.tsne_X[class_member_mask & ~core_samples_mask]\n",
    "            plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                     markeredgecolor='k', markersize=6)\n",
    "\n",
    "        plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(labels, annotation):\n",
    "    true = []\n",
    "    pred = []\n",
    "    \n",
    "    for a, l in zip(annotation, labels):\n",
    "        if not pd.isna(a):\n",
    "            if l == 0:\n",
    "                true.append(0)\n",
    "            else:\n",
    "                true.append(1)\n",
    "            \n",
    "            if a == 'gameplay':\n",
    "                pred.append(0)\n",
    "            else:\n",
    "                pred.append(1)\n",
    "    \n",
    "    print(classification_report(true, pred))\n",
    "    \n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Finished encoding frames using VGG16\n",
      "> Performed dimensionality reduction using TSNE\n",
      "> Created clusters using DBSCAN\n",
      "CPU times: user 17min 18s, sys: 1min 14s, total: 18min 33s\n",
      "Wall time: 2min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "video1_segmentation = ShotSegmentation(os.path.join(BASE_DIR, 'shots'))\n",
    "video1_labels = video1_segmentation.get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Finished encoding frames using VGG16\n",
      "> Performed dimensionality reduction using TSNE\n",
      "> Created clusters using DBSCAN\n",
      "CPU times: user 17min 21s, sys: 1min 13s, total: 18min 35s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "video2_segmentation = ShotSegmentation(os.path.join(BASE_DIR, 'other_transition'))\n",
    "video2_labels = video2_segmentation.get_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "video1_annotation_df = pd.read_csv(os.path.join(BASE_DIR, 'shots_annotation.csv'), header=0)\n",
    "video1_annotation = list(video1_annotation_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "video2_annotation_df = pd.read_csv(os.path.join(BASE_DIR, 'other_transition_annotation.csv'), header=0)\n",
    "video2_annotation = list(video2_annotation_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       116\n",
      "           1       0.98      1.00      0.99        83\n",
      "\n",
      "    accuracy                           0.99       199\n",
      "   macro avg       0.99      0.99      0.99       199\n",
      "weighted avg       0.99      0.99      0.99       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true, pred = get_metrics(list(video1_labels) + list(video2_labels), \n",
    "            list(video1_annotation)+list(video2_annotation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 116, 1: 83})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_folder = os.path.join(base_dir, 'segmentation_results', 'longer_segmentation')\n",
    "# for label, img in zip(db.labels_, frames):\n",
    "#     result_dir = os.path.join(results_folder, str(label))\n",
    "#     result_dest = os.path.join(results_folder, str(label), os.path.split(img)[1])\n",
    "#     if not os.path.exists(result_dir):\n",
    "#         os.makedirs(result_dir)\n",
    "#     copyfile(img, result_dest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
